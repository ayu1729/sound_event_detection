{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frequency_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdzgmrwsEbGnHrzkOm1F/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayu1729/sound_event_detection/blob/main/Frequency_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhPY2yIoX2bt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "3e862239-4e4d-426b-81fc-1d29d34713e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9bb931d6a464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                       \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                       kernel_initializer=\"random_normal\")\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'LazyLoader' object is not callable"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "feature=[[[[0]]]]\n",
        "feature=np.array(feature)\n",
        "cnn_input = tf.expand_dims(feature[:,:,:,0],axis=-1)\n",
        "fa_input = tf.transpose(feature[:,:,:,0],perm=[0,2,1])\n",
        "freq_full1 = tf.compat.v1.layers.dense(inputs=fa_input,\n",
        "                                      units = 40,\n",
        "                                      activation=None,\n",
        "                                      use_bias=True,\n",
        "                                      kernel_initializer=\"random_normal\")\n",
        "                \n",
        "                \n",
        "freq_sigmoid = tf.nn.sigmoid(freq_full1)\n",
        "freq_fnorm = 40*freq_sigmoid/tf.reshape(tf.matmul(tf.reshape(tf.reduce_sum(freq_sigmoid,axis=-1),[-1,1]),tf.ones([1,40])),[feature.shape[0],-1,40])\n",
        "freq_attention = tf.expand_dims(tf.transpose(freq_fnorm,perm=[0,2,1]),axis=-1)\n",
        "        \n",
        "cnn_input = cnn_input*freq_attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "nf=x.shape[3]\n",
        "nc=x.shape[1]\n",
        "nt=x.shape[2]\n",
        "att_freq=k.transpose(x,perm=[0,3,1,2])\n",
        "freq_full1=layers.Dense(units=nf,activation=None,use_bias=True,kernel_initializer='random_normal')(att_freq)\n",
        "freq_sigmoid=k.sigmoid(freq_full1)\n",
        "freq_fnorm =nf*freq_sigmoid/k.repeat_elements(k.sum(freq_sigmoid,axis=-1,keepdims=true),rep=nf,axis=-1)\n",
        "freq_attention=k.transpose(freq_fnorm2,perm=[0,2,3,1])\n",
        "conv1=layers.Multiply()(x,freq_attention)\n",
        "\n"
      ],
      "metadata": {
        "id": "AsVu_SyB5Tbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cnn_input = tf.expand_dims(feature[:,:,:,0],axis=-1)\n",
        "fa_input = tf.transpose(feature[:,:,:,0],perm=[0,2,1])\n",
        "freq_full1 = tf.layers.dense(inputs=fa_input,\n",
        "                                      units = 40,\n",
        "                                      activation=None,\n",
        "                                      use_bias=True,\n",
        "                                      kernel_initializer=initializer,\n",
        "                                      trainable=trainable,\n",
        "                                      name=\"freq_full1\"\n",
        "freq_sigmoid = tf.nn.sigmoid(freq_full1)\n",
        "freq_fnorm = 40*freq_sigmoid/tf.reshape(tf.matmul(tf.reshape(tf.reduce_sum(freq_sigmoid,axis=-1),[-1,1]),tf.ones([1,40])),[batch_size,-1,40])\n",
        "freq_attention = tf.expand_dims(tf.transpose(freq_fnorm,perm=[0,2,1]),axis=-1)\n",
        "cnn_input = cnn_input*freq_attention"
      ],
      "metadata": {
        "id": "S0VV0MCQlbxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_full1 = layers.dense(units = 40, activation=None,\n",
        "                                      use_bias=True,\n",
        "                                      kernel_initializer='random_normal')(fa_input)\n",
        "        \n",
        "                                      "
      ],
      "metadata": {
        "id": "9EXO7_FgrvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, \n",
        "not just on MNIST.\n",
        "*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\n",
        "uncommenting them and commenting their counterparts.\n",
        "\n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "\"\"\"\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers\n",
        "from keras.regularizers import *\n",
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 momentum=0.9,\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.b = None\n",
        "        self.momentum = K.variable(momentum, name='momentum')\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        inputs_expand = K.expand_dims(inputs, 1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
        "\n",
        "        # Begin: Routing algorithm --------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros ONLY FIRST TIME, than recovered from previous iteration\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        if self.b is None:\n",
        "            b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
        "        else:\n",
        "            b = self.b\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "\n",
        "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
        "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                b_update = K.batch_dot(outputs, inputs_hat, [2, 3])\n",
        "                b = self.momentum * b + (1-self.momentum) * b_update\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        self.b = b\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding, cnn_k_regularizer=None, dropout=False,\n",
        "               drop_rate=None):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :param cnn_k_regularizer: regularization type - ADDED BY Fabio\n",
        "    :param dropout: bool\n",
        "    :param dropout: if not None, dropout rate\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels,\n",
        "                           kernel_size=kernel_size,\n",
        "                           strides=strides,\n",
        "                           padding=padding,\n",
        "                           kernel_regularizer=eval(cnn_k_regularizer),\n",
        "                           name='primarycap_conv2d')(inputs)\n",
        "    if dropout:\n",
        "        output = layers.Dropout(drop_rate)(output)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash')(outputs)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# The following is another way to implement primary capsule layer. This is much slower.\n",
        "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    outputs = []\n",
        "    for _ in range(n_channels):\n",
        "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
        "    outputs = layers.Concatenate(axis=1)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "nzvaDIOt4suS",
        "outputId": "b2a78015-c653-4479-be12-a55f21cc1b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# The following is another way to implement primary capsule layer. This is much slower.\\n# Apply Conv2D `n_channels` times and concatenate all capsules\\ndef PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\\n    outputs = []\\n    for _ in range(n_channels):\\n        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\\n        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\\n    outputs = layers.Concatenate(axis=1)(outputs)\\n    return layers.Lambda(squash)(outputs)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ByCnupPN49Hv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}